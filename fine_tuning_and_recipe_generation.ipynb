{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn9HAOZBAQJ7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, AdamW\n",
        "\n",
        "\n",
        "#  Load Pre-trained Model and Tokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "#  Load and Preprocess Data\n",
        "\n",
        "train_path = \"bard_recipes.csv\"  # Update the file name\n",
        "df = pd.read_csv(train_path)\n",
        "texts = df[['Item', 'Recipes']].apply(lambda x: ' '.join(x), axis=1).tolist()\n",
        "tokenized_texts = [tokenizer.encode(text, return_tensors=\"pt\").squeeze() for text in texts]\n",
        "input_ids = torch.cat(tokenized_texts)\n",
        "\n",
        "# Data Collator\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Training Configuration\n",
        "\n",
        "training_args = {\n",
        "    \"output_dir\": \"./recipe_finetuned\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"save_steps\": 10_000,\n",
        "    \"save_total_limit\": 2\n",
        "}\n",
        "# Initialize Optimizer\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, no_deprecation_warning=True)\n",
        "\n",
        "# Training Loop\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_path,\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=training_args[\"per_device_train_batch_size\"],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "for epoch in range(training_args[\"num_train_epochs\"]):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(model.device)\n",
        "        labels = batch[\"labels\"].to(model.device)\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}\")\n",
        "\n",
        "#Save Fine-tuned Model\n",
        "\n",
        "model.save_pretrained(training_args[\"output_dir\"])\n",
        "tokenizer.save_pretrained(training_args[\"output_dir\"])\n",
        "\n",
        "# Load Fine-tuned Model for Recipe Generation\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(training_args[\"output_dir\"])\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(training_args[\"output_dir\"])\n",
        "\n",
        "# Recipe Generation\n",
        "\n",
        "user_input = input(\"Enter the name of the item for which you want a recipe: \")\n",
        "prompt = f\"Item: {user_input}\\nRecipe:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Ensure attention_mask is set\n",
        "attention_mask = torch.ones(input_ids.shape, device=model.device)\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "output = model.generate(input_ids, attention_mask=attention_mask, max_length=200, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "generated_recipe = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nGenerated Recipe for {user_input}:\\n{generated_recipe}\")"
      ]
    }
  ]
}